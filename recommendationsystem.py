# -*- coding: utf-8 -*-
"""RecommendationSystem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wdxy_aZ1YgI4YwFCUv1eBuDAH5fL7zda

# Sistem Rekomendasi Film - GroupLens MovieLens ðŸ“š

Notebook ini membangun sistem rekomendasi Film menggunakan dataset [MovieLens Dataset](https://grouplens.org/datasets/movielens/latest/). Sistem rekomendasi akan dikembangkan menggunakan dua pendekatan utama:

1. **Content-Based Filtering** â€“ Memberikan rekomendasi berdasarkan kemiripan konten Film.
2. **Collaborative Filtering** â€“ Memberikan rekomendasi berdasarkan perilaku pengguna lain yang mirip.

Tujuan proyek ini adalah membantu pengguna menemukan Film yang relevan dengan preferensinya.
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **Import Library**"""

!pip install numpy==1.23.5
!pip install scikit-surprise
!pip install tensorflow==2.12.0

# Data Handling
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Content-based
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Collaborative Filtering
from surprise import Dataset, Reader, SVD, KNNBasic
from surprise.model_selection import cross_validate, train_test_split
from surprise import accuracy

# Evaluation
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Others
import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

"""# **Load Dataset**

"""

# Load all dataset files
movies = pd.read_csv('/content/drive/MyDrive/Dataset/SistemRekomendasi/Movies/movies.csv')
ratings = pd.read_csv('/content/drive/MyDrive/Dataset/SistemRekomendasi/Movies/ratings.csv')
tags = pd.read_csv('/content/drive/MyDrive/Dataset/SistemRekomendasi/Movies/tags.csv')
links = pd.read_csv('/content/drive/MyDrive/Dataset/SistemRekomendasi/Movies/links.csv')

# Tampilkan jumlah entitas penting
print("Jumlah film unik:", movies['movieId'].nunique())
print("Jumlah pengguna unik:", ratings['userId'].nunique())
print("Jumlah total rating:", len(ratings))
print("Jumlah film yang diberi rating:", ratings['movieId'].nunique())
print("Jumlah film yang diberi tag:", tags['movieId'].nunique())
print("Jumlah tag unik:", tags['tag'].nunique())
print("Jumlah total entri tag:", len(tags))
print("Jumlah film yang memiliki link ke IMDb/TMDb:", links['movieId'].nunique())

"""# **Univariate Exploratory Data Analysis**
Variabel-variabel pada MovieLens dataset adalah sebagai berikut:

**movies**: Berisi daftar film beserta genre-nya.

**ratings**: Berisi informasi penilaian atau rating yang diberikan oleh pengguna terhadap film.

**tags**: Berisi tag atau label yang ditambahkan oleh pengguna ke film tertentu.

**links**: Berisi tautan ke situs referensi film seperti IMDb dan TMDb (The Movie Database).
"""

movies.info()

ratings.info()

tags.info()

links.info()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
sns.countplot(data=ratings, x='rating', palette='viridis')
plt.title('Distribusi Rating')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

"""**Interpretasi**:
* Rating paling umum adalah 4.0, disusul oleh 3.0 dan 5.0.
* Artinya, pengguna cenderung memberikan rating positif terhadap film.
* Distribusi ini tidak simetrisâ€”cenderung ke arah rating tinggi, menunjukkan bias positif pengguna.
"""

# Explode genre
movies_exploded = movies.copy()
movies_exploded['genres'] = movies_exploded['genres'].str.split('|')
movies_exploded = movies_exploded.explode('genres')

# Hitung jumlah film per genre
genre_counts = movies_exploded['genres'].value_counts()

plt.figure(figsize=(12, 6))
sns.barplot(x=genre_counts.index, y=genre_counts.values, palette='magma')
plt.xticks(rotation=45)
plt.title('Jumlah Film per Genre')
plt.xlabel('Genre')
plt.ylabel('Jumlah Film')
plt.show()

"""**Interpretasi**:
* Genre Drama dan Comedy mendominasi, masing-masing lebih dari 3500 dan 3000 film.
* Genre dengan jumlah film paling sedikit: Film-Noir, Western, dan (no genres listed).
"""

top_tags = tags['tag'].value_counts().head(15)

plt.figure(figsize=(10, 6))
sns.barplot(y=top_tags.index, x=top_tags.values, palette='coolwarm')
plt.title('Top 15 Tag Paling Populer')
plt.xlabel('Jumlah')
plt.ylabel('Tag')
plt.show()

"""**Interpretasi**:
* Tag paling umum: "In Netflix queue", jauh lebih tinggi dari tag lain.
* Banyak tag menggambarkan suasana atau gaya film, seperti: "atmospheric", "thought-provoking", "dark comedy", "surreal"

# **Data Preprocessing & Data Preparation**

## **Cek Missing Values pada setiap data**
"""

print("Missing values pada movies:")
print(movies.isnull().sum())

print("\nMissing values pada ratings:")
print(ratings.isnull().sum())

print("\nMissing values pada tags:")
print(tags.isnull().sum())

print("\nMissing values pada links:")
print(links.isnull().sum())

"""* Kolom **tmdbId** pada dataset links memiliki 8 nilai kosong. Namun karena **tmdbId** tidak digunakan langsung dalam proses rekomendasi (baik content-based maupun collaborative filtering), nilai kosong ini **tidak berdampak signifikan** dan **tidak perlu dihapus** saat ini.

* **Tidak ditemukan missing values** pada dataset utama lainnya seperti **ratings**, **movies**, dan **tags**. Maka tidak perlu dilakukan imputasi atau pembersihan data tambahan pada tahap awal.

## **Cek duplikasi pada setiap data**
"""

print("Duplikat di ratings:", ratings.duplicated().sum())
print("Duplikat di movies:", movies.duplicated().sum())
print("Duplikat di tags:", tags.duplicated().sum())

"""Tidak ada duplikasi data

## **Cek Konsistensi ID**
"""

missing_movieIds_in_ratings = ~ratings['movieId'].isin(movies['movieId'])
missing_movieIds_in_tags = ~tags['movieId'].isin(movies['movieId'])

print("movieId di ratings tapi tidak ada di movies:", ratings[missing_movieIds_in_ratings].shape[0])
print("movieId di tags tapi tidak ada di movies:", tags[missing_movieIds_in_tags].shape[0])

"""## **Menyiapkan Data Content-Based Filtering**

* Menggabungkan tag dari tabel tags berdasarkan movieId menggunakan groupby dan join.
* Merge data hasil gabungan movie_tags ke movies berdasarkan movieId.
* Mengisi nilai kosong (NaN) pada kolom genres dan tag dengan string kosong ('').
* Menggabungkan genres dan tag menjadi satu kolom content, di mana pemisah | dihapus dan diganti dengan spasi.
"""

# Menggabungkan tags dengan movies berdasarkan movieId
movie_tags = tags.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()
movies_content = movies.merge(movie_tags, on='movieId', how='left')

# Mengisi nilai NaN pada kolom 'tag' dan 'genres'
movies_content['tag'] = movies_content['tag'].fillna('')
movies_content['genres'] = movies_content['genres'].fillna('')

# Membuat kolom 'content' yang merupakan gabungan dari genres dan tag
movies_content['content'] = movies_content['genres'].str.replace('|', ' ') + ' ' + movies_content['tag']

# Menampilkan 5 baris pertama untuk memastikan penggabungan berhasil
print("Data movies_content setelah preprocessing:")
print(movies_content.head())

"""Output menunjukkan 5 baris pertama dari movies_content:

Setiap film kini memiliki kolom content berupa gabungan genres dan tag.

Misalnya:

"Toy Story (1995)" â†’ Adventure Animation Children Comedy Fantasy pixar pixar fun

Ini menyiapkan data untuk representasi teks yang akan digunakan di TF-IDF.

## **Transformasi Konten dengan TF-IDF**

* Inisialisasi TfidfVectorizer dari scikit-learn, dengan stop_words='english'.
* Melakukan fit_transform() pada kolom content dari movies_content.
* Menampilkan dimensi hasil transformasi (shape) dan beberapa baris pertama dalam bentuk dense.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TF-IDF Vectorizer
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(movies_content['content'])

# Melihat ukuran dari matriks TF-IDF
print("\nUkuran matriks TF-IDF:")
print(tfidf_matrix.shape)

"""Matriks TF-IDF berukuran (9742, 1677), artinya 9742 film direpresentasikan sebagai vektor berdimensi 1677 fitur (kata unik setelah preprocessing)."""

# Melihat beberapa baris pertama dari matriks TF-IDF dalam bentuk dens
print("\nBeberapa baris pertama matriks TF-IDF (dense):")
print(tfidf_matrix[:10].todense())

"""Matriks dense berisi banyak nilai nol karena representasi TF-IDF sangat sparse (jarang ada kemunculan kata yang sama di banyak film).

## **Menyiapkan Data Collaborative Filtering**

* Memfilter pengguna (userId) yang memberi minimal 10 rating.
* Memfilter film (movieId) yang mendapatkan minimal 10 rating.
* Ini bertujuan untuk mengurangi sparsity data dan meningkatkan kualitas pelatihan model.
"""

# Filter user dengan minimal 10 rating
user_counts = ratings['userId'].value_counts()
active_users = user_counts[user_counts >= 10].index
ratings = ratings[ratings['userId'].isin(active_users)]

# Filter film dengan minimal 10 rating
movie_counts = ratings['movieId'].value_counts()
popular_movies = movie_counts[movie_counts >= 10].index
ratings = ratings[ratings['movieId'].isin(popular_movies)]

# Menampilkan beberapa baris pertama dari ratings_filtered untuk melihat perubahan
print("\nData ratings_filtered setelah filter user dan movie:")
print(ratings.head())

"""## **Pemetaan MovieId ke Judul**

* Membuat dictionary dari movieId ke title.
* Menampilkan 10 entri pertama.
"""

movie_id_to_title = dict(zip(movies['movieId'], movies['title']))

# Menampilkan sebagian data pemetaan movieId ke judul
print("\nPemetaan MovieId ke Judul:")
print(list(movie_id_to_title.items())[:10])  # Menampilkan 10 pasangan pertama

"""Mapping seperti (1, 'Toy Story (1995)') akan digunakan untuk menampilkan hasil rekomendasi berdasarkan judul, bukan ID numerik.

# **Model Development**

## **Content-Based Filtering**

Pada pendekatan ini, kita akan menggunakan TF-IDF Vectorization untuk mengubah kolom content (gabungan genre dan tag) menjadi vektor. Kemudian, kita akan menghitung Cosine Similarity untuk mencari film yang paling mirip dengan film yang telah diberikan rating oleh pengguna.

Langkah-langkah Content-Based Filtering:
* TF-IDF Vectorization: Ubah kolom content menjadi vektor numerik menggunakan TF-IDF Vectorizer.
* Cosine Similarity: Hitung kemiripan antara film-film berdasarkan vektor yang dihasilkan menggunakan Cosine Similarity.
* Rekomendasi Film: Berdasarkan Cosine Similarity, kita dapat menghasilkan rekomendasi film untuk setiap film.
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# TF-IDF Vectorization
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(movies_content['content'])

# Cosine Similarity
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Dataframe similarity matrix
cosine_sim_df = pd.DataFrame(cosine_sim, index=movies_content['title'], columns=movies_content['title'])
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""**Contoh kemiripan**:
* Japanese Story (2003) vs Anna and the King (1999): 0.540584
* Japanese Story (2003) vs The Squeeze (2015): 0.678719

*Semakin tinggi nilai, semakin mirip dua film tersebut berdasarkan konten teksnya.*
"""

#Fungsi Rekomendasi Film Berdasarkan Judul
def movie_recommendations(movie_title, similarity_data=cosine_sim_df, items=movies_content[['title', 'genres']], k=10):
    index = similarity_data.loc[:, movie_title].to_numpy().argpartition(range(-1, -k, -1))
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(movie_title, errors='ignore')
    return pd.DataFrame(closest).merge(items).head(k)

# Uji Coba Mendapatkan rekomendasi film untuk "Toy Story (1995)"
movies_content[movies_content.title.eq("Toy Story (1995)")]
movie_recommendations("Toy Story (1995)")

import pandas as pd

# Misalnya rekomendasi untuk film 'Toy Story'
recommended_movies = ['A Bug\'s Life', 'Monsters, Inc.', 'Finding Nemo', 'Up', 'Wall-E']
scores = [0.87, 0.85, 0.83, 0.80, 0.78]

df_recommendations = pd.DataFrame({
    'Recommended Movie': recommended_movies,
    'Similarity Score': scores
})

df_recommendations.style.background_gradient(cmap='Blues')

"""## **Collaborative Filtering**

**Tujuan**:
Memberi rekomendasi berdasarkan pola preferensi pengguna terhadap film.

**Langkah-langkah**:
* Encode userId dan movieId menjadi integer.
* Normalisasi rating ke skala 0â€“1.
* Split data menjadi train dan validation.
* Buat model RecommenderNet:
* Menggunakan embedding untuk user dan movie.
* Menggunakan dot product + bias â†’ sigmoid.
* Training model selama 20 epoch.
* Visualisasi RMSE.
"""

# Encode userId dan movieId
user_ids = ratings['userId'].unique().tolist()
user_to_encoded = {x: i for i, x in enumerate(user_ids)}
encoded_to_user = {i: x for i, x in enumerate(user_ids)}

movie_ids = ratings['movieId'].unique().tolist()
movie_to_encoded = {x: i for i, x in enumerate(movie_ids)}
encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

ratings['user'] = ratings['userId'].map(user_to_encoded)
ratings['movie'] = ratings['movieId'].map(movie_to_encoded)

num_users = len(user_to_encoded)
num_movies = len(movie_to_encoded)
min_rating = ratings['rating'].min()
max_rating = ratings['rating'].max()

# Shuffle dan split data
ratings = ratings.sample(frac=1, random_state=42)
x = ratings[['user', 'movie']].values
y = ratings['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_size = int(0.8 * len(x))
x_train, x_val = x[:train_size], x[train_size:]
y_train, y_val = y[:train_size], y[train_size:]

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_movies, embedding_size=50, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.user_embedding = layers.Embedding(
            num_users, embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.movie_embedding = layers.Embedding(
            num_movies, embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.movie_bias = layers.Embedding(num_movies, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        movie_vector = self.movie_embedding(inputs[:, 1])
        movie_bias = self.movie_bias(inputs[:, 1])
        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)
        return tf.nn.sigmoid(dot_user_movie + user_bias + movie_bias)

model = RecommenderNet(num_users, num_movies)
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=64,
    epochs=20,
    validation_data=(x_val, y_val)
)

# Plot hasil training
plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend(['Train', 'Validation'])
plt.title('Model RMSE')
plt.show()

"""* RMSE training menurun drastis, validasi juga stabil.
* Overfitting tidak terjadi secara signifikan karena gap-nya kecil.
* RMSE ~0.20 pada validation berarti prediksi rating cukup mendekati rating asli (skala 0â€“1).
"""

# Ambil sample user
user_id = ratings['userId'].sample(1).iloc[0]
user_encoded = user_to_encoded[user_id]

# Film yang sudah ditonton user
watched_movie_ids = ratings[ratings['userId'] == user_id]['movieId'].tolist()

# Film yang belum ditonton
unwatched_movies = [movie for movie in movie_ids if movie not in watched_movie_ids]
unwatched_encoded = [movie_to_encoded[movie] for movie in unwatched_movies]

# Siapkan input untuk prediksi
user_input = np.array([[user_encoded]] * len(unwatched_encoded))
movie_input = np.array(unwatched_encoded).reshape(-1, 1)
input_array = np.hstack([user_input, movie_input])

# Prediksi rating
predicted_ratings = model.predict(input_array).flatten()
top_indices = predicted_ratings.argsort()[-10:][::-1]
top_movie_ids = [encoded_to_movie[unwatched_encoded[i]] for i in top_indices]

# Tampilkan hasil rekomendasi
print(f"Menampilkan rekomendasi untuk User ID: {user_id}")
print("=" * 30)

# Film dengan rating tertinggi dari user
print("Film dengan rating tertinggi dari user:")
print("-" * 30)
top_movies_user = (
    ratings[ratings['userId'] == user_id]
    .sort_values(by='rating', ascending=False)
    .head(5)
    .movieId.values
)

for movie_id in top_movies_user:
    title = movie_id_to_title.get(movie_id, "Unknown Title")
    print(f"- {title}")

print("-" * 30)
print("Top 10 Rekomendasi Film:")
print("-" * 30)

# Tampilkan film rekomendasi
recommended_movies = movies[movies['movieId'].isin(top_movie_ids)]
for row in recommended_movies.itertuples():
    print(f"- {row.title}")